{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **norm** of a vector is a measure of its length. There are many ways of defining the length of a vector depending on the metric used (i.e., the distance formula chosen). The most common is called the $L_2$ norm, which is computed according to the distance formula you are probably familiar with from grade school. The **$L_2$ norm** of a vector $v$ is denoted by $\\Vert v \\Vert_{2}$ and $\\Vert v \\Vert_{2} = \\sqrt{\\sum_i v_i^2}$. This is sometimes also called Euclidian length and refers to the \"physical\" length of a vector in one-, two-, or three-dimensional space. The $L_1$ norm, or \"Manhattan Distance,\" is computed as $\\Vert v \\Vert_{1} = \\sum_i |v_i|$, and is named after the grid-like road structure in New York City. In general, the **p-norm**, $L_p$, of a vector is $\\Vert v \\Vert_{p} = \\sqrt[p]{(\\sum_i v_i^p)}$. The **$L_\\infty$ norm** is the $p$-norm, where $p = \\infty$. The $L_\\infty$ norm is written as $||v||_\\infty$ and it is equal to the maximum absolute value in $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy import arccos, dot\n",
    "from numpy.linalg import det\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import cond, matrix_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row = np.array([[1, -5, 3, 2, 4]])\n",
    "vector_column = np.array([[1], \n",
    "                          [2], \n",
    "                          [3], \n",
    "                          [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [-5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 4]]\n"
     ]
    }
   ],
   "source": [
    "new_vector = vector_row.T\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_1 is: 15.0\n",
      "L_2 is: 7.4\n",
      "L_inf is: 5.0\n"
     ]
    }
   ],
   "source": [
    "norm_1 = norm(new_vector, 1)\n",
    "norm_2 = norm(new_vector, 2)\n",
    "norm_inf = norm(new_vector, np.inf)\n",
    "print('L_1 is: %.1f'%norm_1)\n",
    "print('L_2 is: %.1f'%norm_2)\n",
    "print('L_inf is: %.1f'%norm_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_1 is: 5.0\n",
      "L_2 is: 7.4\n",
      "L_inf is: 15.0\n"
     ]
    }
   ],
   "source": [
    "norm_1 = norm(vector_row, 1)\n",
    "norm_2 = norm(vector_row, 2)\n",
    "norm_inf = norm(vector_row, np.inf)\n",
    "print('L_1 is: %.1f'%norm_1)\n",
    "print('L_2 is: %.1f'%norm_2)\n",
    "print('L_inf is: %.1f'%norm_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v \\cdot w = \\Vert v \\Vert_{2} \\Vert w \\Vert_{2} \\cos{\\theta}\n",
    "$$\n",
    "\n",
    "The dot product is a measure of how similarly directed the two vectors are. For example, the vectors (1,1) and (2,2) are parallel. If you compute the angle between them using the dot product, you will find that $\\theta = 0$. If the angle between the vectors, $\\theta = \\pi/2$, then the vectors are said to be perpendicular or **orthogonal**, and the dot product is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97992471]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import arccos, dot\n",
    "\n",
    "v = np.array([[10, 9, 3]])\n",
    "w = np.array([[2, 5, 12]])\n",
    "\n",
    "theta = arccos(dot(v, w.T)/(norm(v)*norm(w)))\n",
    "print(theta)\n",
    "\n",
    "# Trigonometric inverse cosine = arccos, element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **cross product** between two vectors, $v$ and $w$, is written $v \\times w$. It is defined by $v \\times w = \\Vert v \\Vert_{2}\\Vert w \\Vert_{2}\\sin{(\\theta)} \\textit{n}$, where $\\theta$ is the angle between the $v$ and $w$ (which can be computed from the dot product) and **$n$** is a vector perpendicular to both $v$ and $w$ with unit length (i.e., the length is one). The geometric interpretation of the cross product is a vector perpendicular to both $v$ and $w$ with length equal to the area enclosed by the parallelogram created by the two vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -6]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[0, 2, 0]])\n",
    "w = np.array([[3, 0, 0]])\n",
    "print(np.cross(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix addition and scalar multiplication for matrices work the same way as for vectors. However, **matrix multiplication** between two matrices, $P$ and $Q$, is defined when $P$ is an ${m} \\times {p}$ matrix and $Q$ is a ${p} \\times {n}$ matrix. The result of $M = PQ$ is a matrix $M$ that is $m \\times n$. The dimension with size $p$ is called the **inner matrix dimension**, and the inner matrix dimensions must match (i.e., the number of columns in $P$ and the number of rows in $Q$ must be the same) for matrix multiplication to be defined. The dimensions $m$ and $n$ are called the **outer matrix dimensions**. Formally, if $P$ is ${m} \\times {p}$ and Q is ${p} \\times {n}$, then $M = PQ$ is defined as\n",
    "\n",
    "$$\n",
    "M_{ij} = \\sum_{k=1}^p P_{ik}Q_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 7]\n",
      " [2 3]\n",
      " [5 0]]\n",
      "[[2 6 3 1]\n",
      " [1 2 3 4]]\n",
      "[[ 9 20 24 29]\n",
      " [ 7 18 15 14]\n",
      " [10 30 15  5]]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[1, 7], [2, 3], [5, 0]])\n",
    "Q = np.array([[2, 6, 3, 1], [1, 2, 3, 4]])\n",
    "print(P)\n",
    "print(Q)\n",
    "print(np.dot(P, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use similar approach to calculate the determinant for higher the dimension of the matrix, but it is much easier to calculate using Python. We will see an example below how to calculate the determinant in Python.  \n",
    "\n",
    "\n",
    "The **identity matrix** is a square matrix with ones on the diagonal and zeros elsewhere. The identity matrix is usually denoted by $I$, and is analagous to the real number identity, 1. That is, multiplying any matrix by $I$ (of compatible size) will produce the same matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: -38.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import det\n",
    "\n",
    "M = np.array([[0,2,1,3], \n",
    "             [3,2,8,1], \n",
    "             [1,0,0,3],\n",
    "             [0,3,2,1]])\n",
    "\n",
    "print('Determinant: %.1f'%det(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "I = np.eye(4)\n",
    "print('I:\\n', I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M*I:\n",
      " [[0. 2. 1. 3.]\n",
      " [3. 2. 8. 1.]\n",
      " [1. 0. 0. 3.]\n",
      " [0. 3. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('M*I:\\n', np.dot(M, I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **inverse** of a square matrix $M$ is a matrix of the same size, $N$, such that $M \\cdot N = I$. The inverse of a matrix is analagous to the inverse of real numbers. For example, the inverse of 3 is $\\frac{1}{3}$ because $(3)(\\frac{1}{3}) = 1$. A matrix is said to be **invertible** if it has an inverse. The inverse of a matrix is unique; that is, for an invertible matrix, there is only one inverse for that matrix. If $M$ is a square matrix, its inverse is denoted by $M^{-1}$ in mathematics, and it can be computed in Python using the function *inv* from Numpy's *linalg* package.\n",
    "\n",
    "For a $2 \\times 2$ matrix, the analytic solution of the matrix inverse is:\n",
    "\n",
    "$$\n",
    "M^{-1} = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\\\\\n",
    "\\end{bmatrix}^{-1} = \\frac{1}{|M|}\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inv M:\n",
      " [[-1.57894737 -0.07894737  1.23684211  1.10526316]\n",
      " [-0.63157895 -0.13157895  0.39473684  0.84210526]\n",
      " [ 0.68421053  0.18421053 -0.55263158 -0.57894737]\n",
      " [ 0.52631579  0.02631579 -0.07894737 -0.36842105]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "print('Inv M:\\n', inv(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix that is close to being singular (i.e., the determinant is close to 0) is called **ill-conditioned**. Although ill-conditioned matrices have inverses, they are problematic numerically in the same way that dividing a number by a very, very small number is problematic. That is, it can result in computations that result in overflow, underflow, or numbers small enough to result in significant round-off errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **condition number** is a measure of how ill-conditioned a matrix is, and it can be computed using Numpy's function *cond* from *linalg*. The higher the condition number, the closer the matrix is to being singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number:\n",
      " 4.048917339522305\n",
      "Rank:\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import cond, matrix_rank\n",
    "\n",
    "A = np.array([[1,1,0],\n",
    "              [0,1,0],\n",
    "              [1,0,1]])\n",
    "\n",
    "print('Condition number:\\n', cond(A))\n",
    "print('Rank:\\n', matrix_rank(A))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is called **full rank**. if rank $(A)=\\min(m,n)$. The matrix, $A$, is also full rank if all of its columns are linearly independent. An **augmented matrix**. is a matrix, $A$, concatenated with a vector, $y$, and is written $[A,y]$. This is commonly read \"$A$ augmented with $y$.\" You can use *np.concatenate* to concatenate the them. If $rank([A,y]) = {rank}(A) + 1$, then the vector, $y$, is \"new\" information. That is, it cannot be created as a linear combination of the columns in $A$. The rank is an important property of matrices because of its relationship to solutions of linear equations, which is discussed in the last section of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented matrix:\n",
      " [[1 1 0 1]\n",
      " [0 1 0 2]\n",
      " [1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[1], [2], [1]])\n",
    "A_y = np.concatenate((A, y), axis = 1)\n",
    "print('Augmented matrix:\\n', A_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "print('Rank:\\n', matrix_rank(A_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
