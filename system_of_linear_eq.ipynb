{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $a_{i,j}$ and $y_i$ are real numbers. The $\\textbf{matrix form}$ of a system of linear equations is $\\textbf{$Ax = y$}$ where $A$ is a ${m} \\times {n}$ matrix, $A(i,j) = a_{i,j}, y$ is a vector in ${\\mathbb{R}}^m$, and $x$ is an unknown vector in ${\\mathbb{R}}^n$. The matrix form is showing as below:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & ... & a_{1,n}\\\\\n",
    "a_{2,1} & a_{2,2} & ... & a_{2,n}\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "a_{m,1} & a_{m,2} & ... & a_{m,n}\n",
    "\\end{bmatrix}\\left[\\begin{array}{c} x_1 \\\\x_2 \\\\ ... \\\\x_n \\end{array}\\right] =\n",
    "\\left[\\begin{array}{c} y_1 \\\\y_2 \\\\ ... \\\\y_m \\end{array}\\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1: There is no solution for $x$.** If ${rank}([A,y]) = {rank}(A) + 1$, then $y$ is\n",
    "linearly independent from the columns of $A$. Therefore $y$ is not in the range of $A$ and by definition, there cannot be an $x$ that satisfies the equation. Thus, comparing rank($[A,y]$) and rank($A$) provides an easy way to check if there are no solutions to a system of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2: There is a unique solution for $x$.** If ${rank}([A,y]) = {rank}(A)$, then $y$ can be written as a linear combination of the columns of $A$ and there is at least one solution for the matrix equation. For there to be only one solution, ${rank}(A) = n$ must also be true. In other words, the number of equations must be exactly equal to the number of unknowns.To see why this property results in a unique solution, consider the following three relationships between $m$ and $n: m < n, m = n$, and $m > n$. \n",
    "\n",
    "* For the case where $m < n$, ${rank}(A) = n$ cannot possibly be true because this means we have a \"fat\" matrix with fewer equations than unknowns. Thus, we do not need to consider this subcase.\n",
    "* When $m = n$ and ${rank}(A) = n$, then $A$ is square and invertible. Since the inverse of a matrix is unique, then the matrix equation $Ax = y$ can be solved by multiplying each side of the equation, on the left, by $A^{-1}$. This results in $A^{-1}Ax = A^{-1}y\\rightarrow Ix = A^{-1}y\\rightarrow x = A^{-1}y$, which gives the unique solution to the equation.\n",
    "* If $m > n$, then there are more equations than unknowns. However if ${rank}(A) = n$, then it is possible to choose $n$ equations (i.e., rows of A) such that if these equations are satisfied, then the remaining $m - n$ equations will be also satisfied. In other words, they are redundant. If the $m-n$ redundant equations are removed from the system, then the resulting system has an $A$ matrix that is $n \\times n$, and invertible. These facts are not proven in this text. The new system then has a unique solution, which is valid for the whole system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 3: There is an infinite number of solutions for $x$.** If ${rank}([A, y]) = {rank}(A)$, then $y$ is in the range of $A$, and there is at least one solution for the matrix equation. However, if rank($A$) $<$ $n$, then there is an infinite number of solutions. The reason for this fact is as follows: although it is not shown here, if rank($A$) $<$ $n$, then there is at least one nonzero vector, $n$, that is in the null space of $A$ (Actually there are an infinite number of null space vectors under these conditions.). If $n$ is in the nullspace of $A$, then $An = 0$ by definition. Now let $x^{{\\ast}}$ be a solution to the matrix equation $Ax = y$; then necessarily, $Ax^{{\\ast}} = y$. However, $Ax^{{\\ast}} + An = y$ or $A(x^{{\\ast}} + n) = y$. Therefore, $x^{{\\ast}} + n$ is also a\n",
    "solution for $Ax = y$. In fact, since $A$ is a linear transformation, $x^{{\\ast}} + \\alpha n$ is a solution for any real number, $\\alpha$ (you should try to show this on your own). Since there are an infinite number of acceptable values for $\\alpha$, there are an infinite number of solutions for the matrix equation.\n",
    "\n",
    "\n",
    "\n",
    "Let's say we have n equations with n variables, $Ax=y$, as shown in the following:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & ... & a_{1,n}\\\\\n",
    "a_{2,1} & a_{2,2} & ... & a_{2,n}\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "a_{n,1} & a_{n,2} & ... & a_{n,n}\n",
    "\\end{bmatrix}\\left[\\begin{array}{c} x_1 \\\\x_2 \\\\ ... \\\\x_n \\end{array}\\right] =\n",
    "\\left[\\begin{array}{c} y_1 \\\\y_2 \\\\ ... \\\\y_n \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Methods - Gauss-Seidel Method\n",
    "\n",
    "The above methods we introduced are all direct methods, in which we compute the solution with a finite number of operations. In this section, we will introduce a different class of methods, the **iterative methods**, or **indirect methods**. It starts with an initial guess of the solution and then repeatedly improve the solution until the change of the solution is below a threshold. In order to use this iterative process, we need first write the explicit form of a system of equations. If we have a system of linear equations:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & ... & a_{1,n}\\\\\n",
    "a_{2,1} & a_{2,2} & ... & a_{2,n}\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "a_{m,1} & a_{m,2} & ... & a_{m,n}\n",
    "\\end{bmatrix}\\left[\\begin{array}{c} x_1 \\\\x_2 \\\\ ... \\\\x_n \\end{array}\\right] =\n",
    "\\left[\\begin{array}{c} y_1 \\\\y_2 \\\\ ... \\\\y_m \\end{array}\\right]$$\n",
    "we can write its explicit form as:\n",
    "\n",
    "$$\n",
    "x_i = \\frac{1}{a_{i,i}}\\Big[y_i - \\sum_{j=1, j \\ne i}^{j=n}{a_{i,j}x_j} \\Big]\n",
    "$$\n",
    "\n",
    "This is the basics of the iterative methods, we can assume initial values for all the $x$, and use it as $x^{(0)}$. In the first iteration, we can substitute $x^{(0)}$ into the right-hand side of the explicit equation above, and get the first iteration solution $x^{(1)}$. Thus, we can substitute $x^{(1)}$ into the equation and get substitute $x^{(2)}$. The iterations continue until the difference between $x^{(k)}$ and $x^{(k-1)}$ is smaller than some pre-defined value. \n",
    "\n",
    "In order to have the iterative methods work, we do need specific condition for the solution to converge. A sufficient but not necessary condition of the convergence is the coefficient matrix $a$ is a **diagonally dominant**. This means that in each row of the matrix of coefficients $a$, the absolute value of the diagonal element is greater than the sum of the absolute values of the off-diagonal elements. If the coefficient matrix satisfy the condition, the iteration will converge to the solution. The solution might still converge even when this condition is not satisfied.\n",
    "\n",
    "### Gauss-Seidel Method\n",
    "The **Gauss-Seidel Method** is a specific iterative method, that is always using the latest estimated value for each elements in $x$. For example, we first assume the initial values for $x_2, x_3, \\cdots, x_n$ (except for $x_1$), and then we can calculate $x_1$. Using the calculated $x_1$ and the rest of the $x$ (except for $x_2$), we can calculate $x_2$. We can continue in the same manner and calculate all the elements in $x$. This will conclude the first iteration. We can see the unique part of Gauss-Seidel method is that we are always using the latest value for calculate the next value in $x$. We can then continue with the iterations until the value converges. Let us use this method to solve the same problem we just solved above. \n",
    "\n",
    "**EXAMPLE:** Solve the following system of linear equations using Gauss-Seidel method, use a pre-defined threshold $\\epsilon = 0.01$. Do remember to check if the converge condition is satisfied or not. \n",
    "\n",
    "\n",
    "8x_1 + 3x_2 - 3x_3 = 14 \n",
    "-2x_1 - 8x_2 + 5x_3 = 5 \n",
    "3x_1 + 5x_2 + 10x_3  =-8\n",
    "\n",
    "\n",
    "Let us first check if the coefficient matrix is diagonally dominant or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix is diagonally dominant\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[8, 3, -3], [-2, -8, 5], [3, 5, 10]]\n",
    "\n",
    "# Find diagonal coefficients\n",
    "diag = np.diag(np.abs(a)) \n",
    "\n",
    "# Find row sum without diagonal\n",
    "off_diag = np.sum(np.abs(a), axis=1) - diag \n",
    "\n",
    "if np.all(diag > off_diag):\n",
    "    print('matrix is diagonally dominant')\n",
    "else:\n",
    "    print('NOT diagonally dominant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration results\n",
      " k,    x1,    x2,    x3 \n",
      "1, 1.7500, -1.0625, 1.5875\n",
      "2, 2.7437, -0.3188, 2.9275\n",
      "3, 2.9673, 0.4629, 3.8433\n",
      "4, 3.0177, 1.0226, 4.4332\n",
      "5, 3.0290, 1.3885, 4.8059\n",
      "6, 3.0315, 1.6208, 5.0397\n",
      "7, 3.0321, 1.7668, 5.1861\n",
      "8, 3.0322, 1.8582, 5.2776\n",
      "9, 3.0322, 1.9154, 5.3348\n",
      "10, 3.0323, 1.9512, 5.3705\n",
      "11, 3.0323, 1.9735, 5.3929\n",
      "12, 3.0323, 1.9875, 5.4068\n",
      "13, 3.0323, 1.9962, 5.4156\n",
      "14, 3.0323, 2.0017, 5.4210\n",
      "Converged!\n"
     ]
    }
   ],
   "source": [
    "x1 = 0\n",
    "x2 = 0\n",
    "x3 = 0\n",
    "epsilon = 0.01\n",
    "converged = False\n",
    "\n",
    "x_old = np.array([x1, x2, x3])\n",
    "\n",
    "print('Iteration results')\n",
    "print(' k,    x1,    x2,    x3 ')\n",
    "for k in range(1, 50):\n",
    "    x1 = (14-3*x2+3*x3)/8\n",
    "    x2 = (5+2*x1-5*x3)/(-8)\n",
    "    x3 = (-8-3*x1-5*x2)/(-5)\n",
    "    x = np.array([x1, x2, x3])\n",
    "    # check if it is smaller than threshold\n",
    "    dx = np.sqrt(np.dot(x-x_old, x-x_old))\n",
    "    \n",
    "    print(\"%d, %.4f, %.4f, %.4f\"%(k, x1, x2, x3))\n",
    "    if dx < epsilon:\n",
    "        converged = True\n",
    "        print('Converged!')\n",
    "        break\n",
    "        \n",
    "    # assign the latest x value to the old value\n",
    "    x_old = x\n",
    "\n",
    "if not converged:\n",
    "    print('Not converge, increase the # of iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4x_1 + 3x_2 - 5x_3 = 2 \n",
    "-2x_1 - 4x_2 + 5x_3 = 5 \n",
    "8x_1 + 8x_2  = -3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the solver is actually doing a LU decomposition to get the results. You can check the help of the function, it needs the input matrix to be square and of full-rank, i.e., all rows (or, equivalently, columns) must be linearly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.20833333 -2.58333333 -0.18333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 3, -5], \n",
    "              [-2, -4, 5], \n",
    "              [8, 8, 0]])\n",
    "y = np.array([2, 5, -3])\n",
    "\n",
    "x = np.linalg.solve(A, y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.20833333 -2.58333333 -0.18333333]\n"
     ]
    }
   ],
   "source": [
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "x = np.dot(A_inv, y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the $L$ and $U$ matrices used in the LU decomposition using the scipy package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "L:\n",
      " [[ 1.    0.    0.  ]\n",
      " [-0.25  1.    0.  ]\n",
      " [ 0.5   0.5   1.  ]]\n",
      "U:\n",
      " [[ 8.   8.   0. ]\n",
      " [ 0.  -2.   5. ]\n",
      " [ 0.   0.  -7.5]]\n",
      "LU:\n",
      " [[ 8.  8.  0.]\n",
      " [-2. -4.  5.]\n",
      " [ 4.  3. -5.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lu\n",
    "\n",
    "P, L, U = lu(A)\n",
    "print('P:\\n', P)\n",
    "print('L:\\n', L)\n",
    "print('U:\\n', U)\n",
    "print('LU:\\n',np.dot(L, U))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
